{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PranavPhanindra/Deep-Learning-PyTorch/blob/main/23357_MTCS_205P_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P8WcNwtbgfM"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci4sQkUSOZdO",
        "outputId": "21637e8b-6efe-49f9-a734-b48faebede35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=c913e0d1401ea7c66a6b0171c77090c42ef3a87232f90566d682d9a80dd499dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz\n",
        "\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G7gdBlqfv_-p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchviz import make_dot\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gsygWWJebsq"
      },
      "source": [
        "# Data Loading and info about Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtS5JpmwegFB",
        "outputId": "ac885fe4-8a8f-4c13-ca0c-66bcd29ab54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15120993.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 270531.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5046849.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13948499.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6QZxzgnViBA0"
      },
      "outputs": [],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "\n",
        "#Defining labels for different classes of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "O3t2B97Se-1i",
        "outputId": "d4265d6f-826f-4ce3-ac82-b711e84fb925"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "img,label = training_data[1]\n",
        "img = img.reshape(28,28)\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.title(labels_map[label])\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LpRbqE71lXBl"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orhS4OKbbxb0"
      },
      "source": [
        "# Creating Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Aun1NS_Hb1R9"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            #Layer 1 - 64 Nodes\n",
        "            #Layer 2 - 128 Nodes\n",
        "            #Relu Activation for each layer\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64 , 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10),\n",
        "        )\n",
        "#Can also declare the layers as linear layer 1 etc..\n",
        "#defining forward function that passes the data on each epoch\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY_U_qwmdxwK"
      },
      "source": [
        "Checking for the devices we have so that we can run the code either on GPU or on CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm4teak5dscY",
        "outputId": "75964be1-e57f-4e60-d64a-12b2fb752752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2VioxzZb8cR",
        "outputId": "c2bb78b7-28d0-476e-ad79-36af1dcf5855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkl7NW2uMVUd"
      },
      "source": [
        "Here we see how the initialisations are done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVfZp4GYHjyP",
        "outputId": "427f738a-ff58-4fe3-de7a-7234fc4c6b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------\n",
            "('0.weight', Parameter containing:\n",
            "tensor([[ 0.0135, -0.0252,  0.0069,  ..., -0.0097,  0.0140,  0.0344],\n",
            "        [-0.0320,  0.0274,  0.0004,  ..., -0.0109,  0.0337, -0.0012],\n",
            "        [ 0.0032,  0.0282,  0.0353,  ...,  0.0074,  0.0113, -0.0087],\n",
            "        ...,\n",
            "        [ 0.0137,  0.0275,  0.0012,  ..., -0.0227,  0.0110, -0.0325],\n",
            "        [ 0.0269, -0.0245,  0.0314,  ..., -0.0220,  0.0260,  0.0079],\n",
            "        [-0.0163,  0.0024, -0.0031,  ...,  0.0124,  0.0155, -0.0341]],\n",
            "       requires_grad=True))\n",
            "torch.Size([64, 784])\n",
            "---------------------------------------------------------------------------------------\n",
            "('0.bias', Parameter containing:\n",
            "tensor([ 0.0197,  0.0066, -0.0118,  0.0093, -0.0036, -0.0323, -0.0028,  0.0272,\n",
            "        -0.0340,  0.0148,  0.0097, -0.0220, -0.0349,  0.0085,  0.0029, -0.0061,\n",
            "         0.0165,  0.0212, -0.0215, -0.0091, -0.0281, -0.0240, -0.0085,  0.0153,\n",
            "         0.0278, -0.0090, -0.0113, -0.0192,  0.0174, -0.0150, -0.0055, -0.0147,\n",
            "        -0.0215, -0.0079,  0.0350, -0.0079, -0.0052,  0.0136,  0.0162, -0.0267,\n",
            "         0.0281,  0.0334,  0.0026,  0.0300,  0.0232,  0.0186, -0.0237, -0.0310,\n",
            "         0.0209, -0.0133,  0.0110,  0.0122,  0.0083,  0.0251, -0.0280, -0.0087,\n",
            "        -0.0323,  0.0218,  0.0012, -0.0193,  0.0289,  0.0314, -0.0222,  0.0188],\n",
            "       requires_grad=True))\n",
            "torch.Size([64])\n",
            "---------------------------------------------------------------------------------------\n",
            "('2.weight', Parameter containing:\n",
            "tensor([[-0.0047, -0.0225,  0.0296,  ...,  0.0036, -0.1092, -0.0698],\n",
            "        [ 0.0016,  0.0863,  0.0197,  ...,  0.1174,  0.1160,  0.0056],\n",
            "        [-0.0957,  0.0289, -0.0730,  ..., -0.1146, -0.0164, -0.0631],\n",
            "        ...,\n",
            "        [ 0.0956,  0.0825,  0.0585,  ..., -0.1249, -0.0325, -0.1106],\n",
            "        [ 0.0419,  0.0056,  0.0320,  ...,  0.0909,  0.0600, -0.0074],\n",
            "        [ 0.0035, -0.0864,  0.0112,  ...,  0.0318,  0.0356, -0.0418]],\n",
            "       requires_grad=True))\n",
            "torch.Size([128, 64])\n",
            "---------------------------------------------------------------------------------------\n",
            "('2.bias', Parameter containing:\n",
            "tensor([ 0.1098,  0.0416, -0.0568,  0.0169,  0.0774, -0.0577,  0.0209, -0.0423,\n",
            "        -0.0639, -0.0397,  0.0947, -0.0054,  0.0275,  0.0814, -0.0205,  0.0760,\n",
            "         0.0442,  0.1216,  0.0123,  0.0397,  0.0009,  0.1175,  0.0618,  0.0155,\n",
            "         0.0570, -0.0452,  0.0300, -0.0112, -0.1055,  0.0639,  0.0407,  0.0786,\n",
            "        -0.1231,  0.1069, -0.0061,  0.1150, -0.0350,  0.0696,  0.0476, -0.0434,\n",
            "         0.0559,  0.0200,  0.1112, -0.0713, -0.0623,  0.0440,  0.0336, -0.1092,\n",
            "         0.1135, -0.0979,  0.0581,  0.1097, -0.0306, -0.0504, -0.0563, -0.1187,\n",
            "        -0.1002,  0.0362,  0.0192, -0.0848,  0.0900, -0.0635, -0.0377,  0.0902,\n",
            "        -0.0162, -0.1106, -0.1217,  0.0266,  0.1161, -0.1204, -0.0399,  0.0932,\n",
            "        -0.0009, -0.0480,  0.0086,  0.1240,  0.0340, -0.1154,  0.0748,  0.0796,\n",
            "         0.0889, -0.0594,  0.0250, -0.0883, -0.0550,  0.0605, -0.0257,  0.0784,\n",
            "        -0.0814,  0.1147, -0.0085,  0.0038,  0.1029, -0.1155,  0.1138, -0.0214,\n",
            "         0.1010,  0.0095, -0.0325,  0.0827,  0.0830,  0.0517, -0.0049, -0.0868,\n",
            "        -0.0564,  0.1107,  0.0970,  0.0709,  0.0270,  0.1117,  0.0988,  0.0454,\n",
            "         0.0274, -0.1146, -0.0224,  0.0537,  0.1217, -0.1207,  0.0548,  0.0445,\n",
            "        -0.0209,  0.0867, -0.0770,  0.0627, -0.0723, -0.0609,  0.0591, -0.0339],\n",
            "       requires_grad=True))\n",
            "torch.Size([128])\n",
            "---------------------------------------------------------------------------------------\n",
            "('4.weight', Parameter containing:\n",
            "tensor([[-0.0756,  0.0243, -0.0476,  ...,  0.0715, -0.0114, -0.0730],\n",
            "        [ 0.0590,  0.0875,  0.0027,  ...,  0.0699,  0.0239,  0.0590],\n",
            "        [-0.0754,  0.0766,  0.0538,  ..., -0.0805,  0.0805, -0.0747],\n",
            "        ...,\n",
            "        [-0.0235, -0.0406,  0.0445,  ..., -0.0635,  0.0857, -0.0112],\n",
            "        [-0.0471,  0.0700, -0.0757,  ..., -0.0430, -0.0611,  0.0669],\n",
            "        [ 0.0455,  0.0151,  0.0138,  ..., -0.0536,  0.0380, -0.0569]],\n",
            "       requires_grad=True))\n",
            "torch.Size([10, 128])\n",
            "---------------------------------------------------------------------------------------\n",
            "('4.bias', Parameter containing:\n",
            "tensor([ 0.0573, -0.0268, -0.0496,  0.0861, -0.0270, -0.0764, -0.0606,  0.0722,\n",
            "         0.0382, -0.0295], requires_grad=True))\n",
            "torch.Size([10])\n",
            "---------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\"\"\"for params in model.parameters() :\n",
        "  print(params)\"\"\"\n",
        "print(\"---------------------------------------------------------------------------------------\")\n",
        "for linParams in model.linear_relu_stack.named_parameters():\n",
        "  print(linParams)\n",
        "  print(linParams[1].size())\n",
        "  print(\"---------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtnLqwdxqCmm"
      },
      "source": [
        "Hyper Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr9MIpmaD-NT"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0pAoa6DOD8k6"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader,model,lossFn,optimizer1) :\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  for batch,(X,y) in enumerate(dataloader) :\n",
        "\n",
        "    #Predictions and loss as we call forward and loss is calculated to be further used\n",
        "    pred = model(X)\n",
        "    make_dot(pred, params=dict(model.named_parameters()))\n",
        "    loss = lossFn(pred,y)\n",
        "\n",
        "    #Backpropagation\n",
        "    #Calculation of Gradient\n",
        "    loss.backward()\n",
        "    #This would update the weights and biases\n",
        "    optimizer1.step()\n",
        "    #This would zero down the gradients so that they arent added up in next step\n",
        "    optimizer1.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "u__E7Qo_U4YS"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    #Set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "COETMezjeWOL"
      },
      "outputs": [],
      "source": [
        "learningRate = 1e-1\n",
        "batchSize  = 100\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p6WB1SJ0h2eH"
      },
      "outputs": [],
      "source": [
        "#Combines LogSoftmax and NLLLoss - Negativce log likelihood\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "optimizer1 = torch.optim.SGD(model.parameters(),lr = learningRate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtPnVfzsVubJ",
        "outputId": "2e1fce2c-2178-41cc-e702-b6cda7d9269a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.289913  [   64/60000]\n",
            "loss: 0.958629  [ 6464/60000]\n",
            "loss: 0.605377  [12864/60000]\n",
            "loss: 0.714884  [19264/60000]\n",
            "loss: 0.632554  [25664/60000]\n",
            "loss: 0.530442  [32064/60000]\n",
            "loss: 0.565547  [38464/60000]\n",
            "loss: 0.606861  [44864/60000]\n",
            "loss: 0.604545  [51264/60000]\n",
            "loss: 0.467601  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.559647 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.461893  [   64/60000]\n",
            "loss: 0.480949  [ 6464/60000]\n",
            "loss: 0.362726  [12864/60000]\n",
            "loss: 0.443346  [19264/60000]\n",
            "loss: 0.459739  [25664/60000]\n",
            "loss: 0.440190  [32064/60000]\n",
            "loss: 0.429392  [38464/60000]\n",
            "loss: 0.540406  [44864/60000]\n",
            "loss: 0.551607  [51264/60000]\n",
            "loss: 0.415861  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.468016 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.326799  [   64/60000]\n",
            "loss: 0.397830  [ 6464/60000]\n",
            "loss: 0.310801  [12864/60000]\n",
            "loss: 0.372447  [19264/60000]\n",
            "loss: 0.374602  [25664/60000]\n",
            "loss: 0.410297  [32064/60000]\n",
            "loss: 0.368327  [38464/60000]\n",
            "loss: 0.487218  [44864/60000]\n",
            "loss: 0.502275  [51264/60000]\n",
            "loss: 0.413567  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.434013 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.264622  [   64/60000]\n",
            "loss: 0.355963  [ 6464/60000]\n",
            "loss: 0.271422  [12864/60000]\n",
            "loss: 0.341817  [19264/60000]\n",
            "loss: 0.320351  [25664/60000]\n",
            "loss: 0.392769  [32064/60000]\n",
            "loss: 0.341510  [38464/60000]\n",
            "loss: 0.444859  [44864/60000]\n",
            "loss: 0.434401  [51264/60000]\n",
            "loss: 0.422406  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.411406 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.233433  [   64/60000]\n",
            "loss: 0.340219  [ 6464/60000]\n",
            "loss: 0.242243  [12864/60000]\n",
            "loss: 0.316866  [19264/60000]\n",
            "loss: 0.319562  [25664/60000]\n",
            "loss: 0.369370  [32064/60000]\n",
            "loss: 0.310870  [38464/60000]\n",
            "loss: 0.426279  [44864/60000]\n",
            "loss: 0.413207  [51264/60000]\n",
            "loss: 0.389336  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.4%, Avg loss: 0.395832 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.211034  [   64/60000]\n",
            "loss: 0.325687  [ 6464/60000]\n",
            "loss: 0.212815  [12864/60000]\n",
            "loss: 0.300550  [19264/60000]\n",
            "loss: 0.297800  [25664/60000]\n",
            "loss: 0.351764  [32064/60000]\n",
            "loss: 0.295027  [38464/60000]\n",
            "loss: 0.397721  [44864/60000]\n",
            "loss: 0.385645  [51264/60000]\n",
            "loss: 0.379450  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.372899 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.192353  [   64/60000]\n",
            "loss: 0.314730  [ 6464/60000]\n",
            "loss: 0.206464  [12864/60000]\n",
            "loss: 0.292694  [19264/60000]\n",
            "loss: 0.306715  [25664/60000]\n",
            "loss: 0.334054  [32064/60000]\n",
            "loss: 0.283515  [38464/60000]\n",
            "loss: 0.378519  [44864/60000]\n",
            "loss: 0.357149  [51264/60000]\n",
            "loss: 0.369296  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.370213 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.193186  [   64/60000]\n",
            "loss: 0.317247  [ 6464/60000]\n",
            "loss: 0.198092  [12864/60000]\n",
            "loss: 0.292032  [19264/60000]\n",
            "loss: 0.303414  [25664/60000]\n",
            "loss: 0.315747  [32064/60000]\n",
            "loss: 0.290629  [38464/60000]\n",
            "loss: 0.358114  [44864/60000]\n",
            "loss: 0.339063  [51264/60000]\n",
            "loss: 0.374246  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 0.368159 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.189452  [   64/60000]\n",
            "loss: 0.318597  [ 6464/60000]\n",
            "loss: 0.187657  [12864/60000]\n",
            "loss: 0.272399  [19264/60000]\n",
            "loss: 0.302540  [25664/60000]\n",
            "loss: 0.311410  [32064/60000]\n",
            "loss: 0.281463  [38464/60000]\n",
            "loss: 0.343402  [44864/60000]\n",
            "loss: 0.343321  [51264/60000]\n",
            "loss: 0.363411  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.360773 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.171585  [   64/60000]\n",
            "loss: 0.312297  [ 6464/60000]\n",
            "loss: 0.181844  [12864/60000]\n",
            "loss: 0.263598  [19264/60000]\n",
            "loss: 0.286381  [25664/60000]\n",
            "loss: 0.295600  [32064/60000]\n",
            "loss: 0.276401  [38464/60000]\n",
            "loss: 0.324800  [44864/60000]\n",
            "loss: 0.336305  [51264/60000]\n",
            "loss: 0.343140  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.352486 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.176431  [   64/60000]\n",
            "loss: 0.306236  [ 6464/60000]\n",
            "loss: 0.174295  [12864/60000]\n",
            "loss: 0.266352  [19264/60000]\n",
            "loss: 0.290451  [25664/60000]\n",
            "loss: 0.300387  [32064/60000]\n",
            "loss: 0.279186  [38464/60000]\n",
            "loss: 0.335209  [44864/60000]\n",
            "loss: 0.334909  [51264/60000]\n",
            "loss: 0.353511  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.346241 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.155306  [   64/60000]\n",
            "loss: 0.302462  [ 6464/60000]\n",
            "loss: 0.160729  [12864/60000]\n",
            "loss: 0.254886  [19264/60000]\n",
            "loss: 0.284498  [25664/60000]\n",
            "loss: 0.290810  [32064/60000]\n",
            "loss: 0.269624  [38464/60000]\n",
            "loss: 0.324798  [44864/60000]\n",
            "loss: 0.299987  [51264/60000]\n",
            "loss: 0.336111  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.343296 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.152422  [   64/60000]\n",
            "loss: 0.283993  [ 6464/60000]\n",
            "loss: 0.159658  [12864/60000]\n",
            "loss: 0.239736  [19264/60000]\n",
            "loss: 0.286609  [25664/60000]\n",
            "loss: 0.283226  [32064/60000]\n",
            "loss: 0.248332  [38464/60000]\n",
            "loss: 0.314761  [44864/60000]\n",
            "loss: 0.286625  [51264/60000]\n",
            "loss: 0.327201  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.339066 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.148381  [   64/60000]\n",
            "loss: 0.280635  [ 6464/60000]\n",
            "loss: 0.165455  [12864/60000]\n",
            "loss: 0.215883  [19264/60000]\n",
            "loss: 0.294688  [25664/60000]\n",
            "loss: 0.277608  [32064/60000]\n",
            "loss: 0.255448  [38464/60000]\n",
            "loss: 0.311865  [44864/60000]\n",
            "loss: 0.280954  [51264/60000]\n",
            "loss: 0.328697  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.337070 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.143339  [   64/60000]\n",
            "loss: 0.282620  [ 6464/60000]\n",
            "loss: 0.157084  [12864/60000]\n",
            "loss: 0.218940  [19264/60000]\n",
            "loss: 0.282583  [25664/60000]\n",
            "loss: 0.279486  [32064/60000]\n",
            "loss: 0.257831  [38464/60000]\n",
            "loss: 0.296328  [44864/60000]\n",
            "loss: 0.263406  [51264/60000]\n",
            "loss: 0.320996  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.340413 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.146379  [   64/60000]\n",
            "loss: 0.278021  [ 6464/60000]\n",
            "loss: 0.154093  [12864/60000]\n",
            "loss: 0.211102  [19264/60000]\n",
            "loss: 0.262988  [25664/60000]\n",
            "loss: 0.274016  [32064/60000]\n",
            "loss: 0.257177  [38464/60000]\n",
            "loss: 0.292923  [44864/60000]\n",
            "loss: 0.249516  [51264/60000]\n",
            "loss: 0.301033  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.336891 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.144075  [   64/60000]\n",
            "loss: 0.285458  [ 6464/60000]\n",
            "loss: 0.143312  [12864/60000]\n",
            "loss: 0.206807  [19264/60000]\n",
            "loss: 0.274580  [25664/60000]\n",
            "loss: 0.274529  [32064/60000]\n",
            "loss: 0.258990  [38464/60000]\n",
            "loss: 0.298233  [44864/60000]\n",
            "loss: 0.255752  [51264/60000]\n",
            "loss: 0.289780  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.346429 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.139784  [   64/60000]\n",
            "loss: 0.273949  [ 6464/60000]\n",
            "loss: 0.159935  [12864/60000]\n",
            "loss: 0.218546  [19264/60000]\n",
            "loss: 0.264553  [25664/60000]\n",
            "loss: 0.271646  [32064/60000]\n",
            "loss: 0.240220  [38464/60000]\n",
            "loss: 0.303078  [44864/60000]\n",
            "loss: 0.243506  [51264/60000]\n",
            "loss: 0.292107  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.348431 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.140592  [   64/60000]\n",
            "loss: 0.271321  [ 6464/60000]\n",
            "loss: 0.163560  [12864/60000]\n",
            "loss: 0.214037  [19264/60000]\n",
            "loss: 0.267630  [25664/60000]\n",
            "loss: 0.272619  [32064/60000]\n",
            "loss: 0.273631  [38464/60000]\n",
            "loss: 0.304418  [44864/60000]\n",
            "loss: 0.234711  [51264/60000]\n",
            "loss: 0.270639  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.348513 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.145622  [   64/60000]\n",
            "loss: 0.274877  [ 6464/60000]\n",
            "loss: 0.146444  [12864/60000]\n",
            "loss: 0.202298  [19264/60000]\n",
            "loss: 0.250663  [25664/60000]\n",
            "loss: 0.248205  [32064/60000]\n",
            "loss: 0.237349  [38464/60000]\n",
            "loss: 0.291732  [44864/60000]\n",
            "loss: 0.226735  [51264/60000]\n",
            "loss: 0.270352  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.349977 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.136575  [   64/60000]\n",
            "loss: 0.271219  [ 6464/60000]\n",
            "loss: 0.150217  [12864/60000]\n",
            "loss: 0.186993  [19264/60000]\n",
            "loss: 0.254970  [25664/60000]\n",
            "loss: 0.248832  [32064/60000]\n",
            "loss: 0.271889  [38464/60000]\n",
            "loss: 0.297831  [44864/60000]\n",
            "loss: 0.217659  [51264/60000]\n",
            "loss: 0.244134  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.353162 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.123434  [   64/60000]\n",
            "loss: 0.278407  [ 6464/60000]\n",
            "loss: 0.155011  [12864/60000]\n",
            "loss: 0.183778  [19264/60000]\n",
            "loss: 0.259046  [25664/60000]\n",
            "loss: 0.242065  [32064/60000]\n",
            "loss: 0.236424  [38464/60000]\n",
            "loss: 0.279239  [44864/60000]\n",
            "loss: 0.212464  [51264/60000]\n",
            "loss: 0.236781  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.355079 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.127978  [   64/60000]\n",
            "loss: 0.274559  [ 6464/60000]\n",
            "loss: 0.157523  [12864/60000]\n",
            "loss: 0.181616  [19264/60000]\n",
            "loss: 0.247777  [25664/60000]\n",
            "loss: 0.245788  [32064/60000]\n",
            "loss: 0.250032  [38464/60000]\n",
            "loss: 0.281093  [44864/60000]\n",
            "loss: 0.199015  [51264/60000]\n",
            "loss: 0.215648  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.367066 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.122479  [   64/60000]\n",
            "loss: 0.274348  [ 6464/60000]\n",
            "loss: 0.156357  [12864/60000]\n",
            "loss: 0.166709  [19264/60000]\n",
            "loss: 0.244909  [25664/60000]\n",
            "loss: 0.260506  [32064/60000]\n",
            "loss: 0.203033  [38464/60000]\n",
            "loss: 0.290732  [44864/60000]\n",
            "loss: 0.178594  [51264/60000]\n",
            "loss: 0.207236  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.376183 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.114088  [   64/60000]\n",
            "loss: 0.277715  [ 6464/60000]\n",
            "loss: 0.148972  [12864/60000]\n",
            "loss: 0.170561  [19264/60000]\n",
            "loss: 0.229375  [25664/60000]\n",
            "loss: 0.256025  [32064/60000]\n",
            "loss: 0.198641  [38464/60000]\n",
            "loss: 0.272322  [44864/60000]\n",
            "loss: 0.196519  [51264/60000]\n",
            "loss: 0.212372  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.370732 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.129926  [   64/60000]\n",
            "loss: 0.276528  [ 6464/60000]\n",
            "loss: 0.141641  [12864/60000]\n",
            "loss: 0.158154  [19264/60000]\n",
            "loss: 0.236427  [25664/60000]\n",
            "loss: 0.263877  [32064/60000]\n",
            "loss: 0.198629  [38464/60000]\n",
            "loss: 0.293030  [44864/60000]\n",
            "loss: 0.179312  [51264/60000]\n",
            "loss: 0.202191  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.373385 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.129698  [   64/60000]\n",
            "loss: 0.270864  [ 6464/60000]\n",
            "loss: 0.136582  [12864/60000]\n",
            "loss: 0.155446  [19264/60000]\n",
            "loss: 0.228853  [25664/60000]\n",
            "loss: 0.239336  [32064/60000]\n",
            "loss: 0.183503  [38464/60000]\n",
            "loss: 0.283110  [44864/60000]\n",
            "loss: 0.186762  [51264/60000]\n",
            "loss: 0.191599  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.371001 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.124398  [   64/60000]\n",
            "loss: 0.275110  [ 6464/60000]\n",
            "loss: 0.146367  [12864/60000]\n",
            "loss: 0.143511  [19264/60000]\n",
            "loss: 0.222209  [25664/60000]\n",
            "loss: 0.253949  [32064/60000]\n",
            "loss: 0.179611  [38464/60000]\n",
            "loss: 0.266069  [44864/60000]\n",
            "loss: 0.187804  [51264/60000]\n",
            "loss: 0.183971  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.381317 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.126546  [   64/60000]\n",
            "loss: 0.274409  [ 6464/60000]\n",
            "loss: 0.127159  [12864/60000]\n",
            "loss: 0.146946  [19264/60000]\n",
            "loss: 0.226437  [25664/60000]\n",
            "loss: 0.256058  [32064/60000]\n",
            "loss: 0.196677  [38464/60000]\n",
            "loss: 0.248335  [44864/60000]\n",
            "loss: 0.181615  [51264/60000]\n",
            "loss: 0.167017  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.380446 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.127853  [   64/60000]\n",
            "loss: 0.265861  [ 6464/60000]\n",
            "loss: 0.128211  [12864/60000]\n",
            "loss: 0.153101  [19264/60000]\n",
            "loss: 0.213971  [25664/60000]\n",
            "loss: 0.248574  [32064/60000]\n",
            "loss: 0.189464  [38464/60000]\n",
            "loss: 0.272950  [44864/60000]\n",
            "loss: 0.173407  [51264/60000]\n",
            "loss: 0.168790  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.384623 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.125061  [   64/60000]\n",
            "loss: 0.273569  [ 6464/60000]\n",
            "loss: 0.120976  [12864/60000]\n",
            "loss: 0.150080  [19264/60000]\n",
            "loss: 0.223435  [25664/60000]\n",
            "loss: 0.239268  [32064/60000]\n",
            "loss: 0.176223  [38464/60000]\n",
            "loss: 0.226426  [44864/60000]\n",
            "loss: 0.172909  [51264/60000]\n",
            "loss: 0.171058  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.393177 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.145614  [   64/60000]\n",
            "loss: 0.256686  [ 6464/60000]\n",
            "loss: 0.125853  [12864/60000]\n",
            "loss: 0.160344  [19264/60000]\n",
            "loss: 0.211780  [25664/60000]\n",
            "loss: 0.238810  [32064/60000]\n",
            "loss: 0.181972  [38464/60000]\n",
            "loss: 0.249173  [44864/60000]\n",
            "loss: 0.157146  [51264/60000]\n",
            "loss: 0.158044  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.392871 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.108460  [   64/60000]\n",
            "loss: 0.283262  [ 6464/60000]\n",
            "loss: 0.114091  [12864/60000]\n",
            "loss: 0.121974  [19264/60000]\n",
            "loss: 0.235799  [25664/60000]\n",
            "loss: 0.249652  [32064/60000]\n",
            "loss: 0.183012  [38464/60000]\n",
            "loss: 0.228568  [44864/60000]\n",
            "loss: 0.154644  [51264/60000]\n",
            "loss: 0.166845  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.395225 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.129042  [   64/60000]\n",
            "loss: 0.281582  [ 6464/60000]\n",
            "loss: 0.126371  [12864/60000]\n",
            "loss: 0.134575  [19264/60000]\n",
            "loss: 0.215445  [25664/60000]\n",
            "loss: 0.224835  [32064/60000]\n",
            "loss: 0.162602  [38464/60000]\n",
            "loss: 0.209431  [44864/60000]\n",
            "loss: 0.141957  [51264/60000]\n",
            "loss: 0.164762  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.417476 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.111680  [   64/60000]\n",
            "loss: 0.264833  [ 6464/60000]\n",
            "loss: 0.122145  [12864/60000]\n",
            "loss: 0.135650  [19264/60000]\n",
            "loss: 0.186365  [25664/60000]\n",
            "loss: 0.224845  [32064/60000]\n",
            "loss: 0.156198  [38464/60000]\n",
            "loss: 0.187227  [44864/60000]\n",
            "loss: 0.163539  [51264/60000]\n",
            "loss: 0.168932  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.414807 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.145667  [   64/60000]\n",
            "loss: 0.246707  [ 6464/60000]\n",
            "loss: 0.103966  [12864/60000]\n",
            "loss: 0.130112  [19264/60000]\n",
            "loss: 0.188110  [25664/60000]\n",
            "loss: 0.219117  [32064/60000]\n",
            "loss: 0.156825  [38464/60000]\n",
            "loss: 0.168356  [44864/60000]\n",
            "loss: 0.135008  [51264/60000]\n",
            "loss: 0.125005  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.413981 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.109140  [   64/60000]\n",
            "loss: 0.242036  [ 6464/60000]\n",
            "loss: 0.113628  [12864/60000]\n",
            "loss: 0.134386  [19264/60000]\n",
            "loss: 0.183535  [25664/60000]\n",
            "loss: 0.229751  [32064/60000]\n",
            "loss: 0.173478  [38464/60000]\n",
            "loss: 0.163486  [44864/60000]\n",
            "loss: 0.123638  [51264/60000]\n",
            "loss: 0.154806  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.413698 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.127248  [   64/60000]\n",
            "loss: 0.237514  [ 6464/60000]\n",
            "loss: 0.091316  [12864/60000]\n",
            "loss: 0.108495  [19264/60000]\n",
            "loss: 0.171839  [25664/60000]\n",
            "loss: 0.224058  [32064/60000]\n",
            "loss: 0.147138  [38464/60000]\n",
            "loss: 0.154326  [44864/60000]\n",
            "loss: 0.124037  [51264/60000]\n",
            "loss: 0.160110  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.420408 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.130192  [   64/60000]\n",
            "loss: 0.255935  [ 6464/60000]\n",
            "loss: 0.090772  [12864/60000]\n",
            "loss: 0.120032  [19264/60000]\n",
            "loss: 0.171043  [25664/60000]\n",
            "loss: 0.228950  [32064/60000]\n",
            "loss: 0.129232  [38464/60000]\n",
            "loss: 0.149978  [44864/60000]\n",
            "loss: 0.113820  [51264/60000]\n",
            "loss: 0.139504  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.434559 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.137258  [   64/60000]\n",
            "loss: 0.233737  [ 6464/60000]\n",
            "loss: 0.095576  [12864/60000]\n",
            "loss: 0.103784  [19264/60000]\n",
            "loss: 0.198534  [25664/60000]\n",
            "loss: 0.204517  [32064/60000]\n",
            "loss: 0.110161  [38464/60000]\n",
            "loss: 0.174679  [44864/60000]\n",
            "loss: 0.103760  [51264/60000]\n",
            "loss: 0.136115  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.423542 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.093121  [   64/60000]\n",
            "loss: 0.227933  [ 6464/60000]\n",
            "loss: 0.104167  [12864/60000]\n",
            "loss: 0.100650  [19264/60000]\n",
            "loss: 0.163742  [25664/60000]\n",
            "loss: 0.197145  [32064/60000]\n",
            "loss: 0.137663  [38464/60000]\n",
            "loss: 0.120139  [44864/60000]\n",
            "loss: 0.122941  [51264/60000]\n",
            "loss: 0.118757  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.418736 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.116762  [   64/60000]\n",
            "loss: 0.230697  [ 6464/60000]\n",
            "loss: 0.081068  [12864/60000]\n",
            "loss: 0.081424  [19264/60000]\n",
            "loss: 0.175155  [25664/60000]\n",
            "loss: 0.192545  [32064/60000]\n",
            "loss: 0.131562  [38464/60000]\n",
            "loss: 0.141389  [44864/60000]\n",
            "loss: 0.127932  [51264/60000]\n",
            "loss: 0.147709  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.427622 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.138175  [   64/60000]\n",
            "loss: 0.264301  [ 6464/60000]\n",
            "loss: 0.092912  [12864/60000]\n",
            "loss: 0.089852  [19264/60000]\n",
            "loss: 0.169694  [25664/60000]\n",
            "loss: 0.188136  [32064/60000]\n",
            "loss: 0.129166  [38464/60000]\n",
            "loss: 0.154106  [44864/60000]\n",
            "loss: 0.112674  [51264/60000]\n",
            "loss: 0.123864  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.451479 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.114777  [   64/60000]\n",
            "loss: 0.250214  [ 6464/60000]\n",
            "loss: 0.094801  [12864/60000]\n",
            "loss: 0.104263  [19264/60000]\n",
            "loss: 0.158922  [25664/60000]\n",
            "loss: 0.200025  [32064/60000]\n",
            "loss: 0.123451  [38464/60000]\n",
            "loss: 0.123373  [44864/60000]\n",
            "loss: 0.111274  [51264/60000]\n",
            "loss: 0.115215  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.446573 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.123041  [   64/60000]\n",
            "loss: 0.226270  [ 6464/60000]\n",
            "loss: 0.078059  [12864/60000]\n",
            "loss: 0.101744  [19264/60000]\n",
            "loss: 0.201113  [25664/60000]\n",
            "loss: 0.175502  [32064/60000]\n",
            "loss: 0.109560  [38464/60000]\n",
            "loss: 0.138458  [44864/60000]\n",
            "loss: 0.116828  [51264/60000]\n",
            "loss: 0.135769  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.455328 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.125458  [   64/60000]\n",
            "loss: 0.238183  [ 6464/60000]\n",
            "loss: 0.091953  [12864/60000]\n",
            "loss: 0.094131  [19264/60000]\n",
            "loss: 0.226986  [25664/60000]\n",
            "loss: 0.194213  [32064/60000]\n",
            "loss: 0.111335  [38464/60000]\n",
            "loss: 0.121124  [44864/60000]\n",
            "loss: 0.122111  [51264/60000]\n",
            "loss: 0.114530  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.463632 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.117411  [   64/60000]\n",
            "loss: 0.250383  [ 6464/60000]\n",
            "loss: 0.087048  [12864/60000]\n",
            "loss: 0.100716  [19264/60000]\n",
            "loss: 0.219929  [25664/60000]\n",
            "loss: 0.196259  [32064/60000]\n",
            "loss: 0.162701  [38464/60000]\n",
            "loss: 0.150274  [44864/60000]\n",
            "loss: 0.108022  [51264/60000]\n",
            "loss: 0.119769  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.454025 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.106552  [   64/60000]\n",
            "loss: 0.227699  [ 6464/60000]\n",
            "loss: 0.093572  [12864/60000]\n",
            "loss: 0.100191  [19264/60000]\n",
            "loss: 0.211638  [25664/60000]\n",
            "loss: 0.214901  [32064/60000]\n",
            "loss: 0.110533  [38464/60000]\n",
            "loss: 0.122685  [44864/60000]\n",
            "loss: 0.106781  [51264/60000]\n",
            "loss: 0.127648  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.458215 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.095420  [   64/60000]\n",
            "loss: 0.245705  [ 6464/60000]\n",
            "loss: 0.069841  [12864/60000]\n",
            "loss: 0.072111  [19264/60000]\n",
            "loss: 0.237756  [25664/60000]\n",
            "loss: 0.192954  [32064/60000]\n",
            "loss: 0.122815  [38464/60000]\n",
            "loss: 0.128017  [44864/60000]\n",
            "loss: 0.101693  [51264/60000]\n",
            "loss: 0.111223  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.475801 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.099595  [   64/60000]\n",
            "loss: 0.206853  [ 6464/60000]\n",
            "loss: 0.069332  [12864/60000]\n",
            "loss: 0.097151  [19264/60000]\n",
            "loss: 0.207731  [25664/60000]\n",
            "loss: 0.170501  [32064/60000]\n",
            "loss: 0.107252  [38464/60000]\n",
            "loss: 0.131125  [44864/60000]\n",
            "loss: 0.105590  [51264/60000]\n",
            "loss: 0.131304  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.478302 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.097684  [   64/60000]\n",
            "loss: 0.240398  [ 6464/60000]\n",
            "loss: 0.074173  [12864/60000]\n",
            "loss: 0.095259  [19264/60000]\n",
            "loss: 0.255123  [25664/60000]\n",
            "loss: 0.149633  [32064/60000]\n",
            "loss: 0.077961  [38464/60000]\n",
            "loss: 0.135453  [44864/60000]\n",
            "loss: 0.110697  [51264/60000]\n",
            "loss: 0.196404  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.462178 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.097303  [   64/60000]\n",
            "loss: 0.217151  [ 6464/60000]\n",
            "loss: 0.068574  [12864/60000]\n",
            "loss: 0.086098  [19264/60000]\n",
            "loss: 0.237389  [25664/60000]\n",
            "loss: 0.168341  [32064/60000]\n",
            "loss: 0.131902  [38464/60000]\n",
            "loss: 0.132178  [44864/60000]\n",
            "loss: 0.107446  [51264/60000]\n",
            "loss: 0.128601  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.483127 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.087450  [   64/60000]\n",
            "loss: 0.229397  [ 6464/60000]\n",
            "loss: 0.058777  [12864/60000]\n",
            "loss: 0.076731  [19264/60000]\n",
            "loss: 0.250396  [25664/60000]\n",
            "loss: 0.172654  [32064/60000]\n",
            "loss: 0.102117  [38464/60000]\n",
            "loss: 0.116282  [44864/60000]\n",
            "loss: 0.135205  [51264/60000]\n",
            "loss: 0.108240  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.495742 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.103216  [   64/60000]\n",
            "loss: 0.238299  [ 6464/60000]\n",
            "loss: 0.048392  [12864/60000]\n",
            "loss: 0.076801  [19264/60000]\n",
            "loss: 0.223787  [25664/60000]\n",
            "loss: 0.191965  [32064/60000]\n",
            "loss: 0.095865  [38464/60000]\n",
            "loss: 0.129379  [44864/60000]\n",
            "loss: 0.122496  [51264/60000]\n",
            "loss: 0.163925  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.503672 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.109711  [   64/60000]\n",
            "loss: 0.236994  [ 6464/60000]\n",
            "loss: 0.061456  [12864/60000]\n",
            "loss: 0.088463  [19264/60000]\n",
            "loss: 0.181621  [25664/60000]\n",
            "loss: 0.170501  [32064/60000]\n",
            "loss: 0.077130  [38464/60000]\n",
            "loss: 0.132967  [44864/60000]\n",
            "loss: 0.116931  [51264/60000]\n",
            "loss: 0.131426  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.524388 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.109432  [   64/60000]\n",
            "loss: 0.210069  [ 6464/60000]\n",
            "loss: 0.071624  [12864/60000]\n",
            "loss: 0.063687  [19264/60000]\n",
            "loss: 0.189476  [25664/60000]\n",
            "loss: 0.134923  [32064/60000]\n",
            "loss: 0.099812  [38464/60000]\n",
            "loss: 0.118162  [44864/60000]\n",
            "loss: 0.124378  [51264/60000]\n",
            "loss: 0.160438  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.501578 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.097314  [   64/60000]\n",
            "loss: 0.212798  [ 6464/60000]\n",
            "loss: 0.068938  [12864/60000]\n",
            "loss: 0.086874  [19264/60000]\n",
            "loss: 0.134299  [25664/60000]\n",
            "loss: 0.184740  [32064/60000]\n",
            "loss: 0.094820  [38464/60000]\n",
            "loss: 0.143456  [44864/60000]\n",
            "loss: 0.109637  [51264/60000]\n",
            "loss: 0.077437  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.505063 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.090332  [   64/60000]\n",
            "loss: 0.221990  [ 6464/60000]\n",
            "loss: 0.067385  [12864/60000]\n",
            "loss: 0.073786  [19264/60000]\n",
            "loss: 0.205338  [25664/60000]\n",
            "loss: 0.167285  [32064/60000]\n",
            "loss: 0.071921  [38464/60000]\n",
            "loss: 0.118201  [44864/60000]\n",
            "loss: 0.103567  [51264/60000]\n",
            "loss: 0.106838  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.521956 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.094180  [   64/60000]\n",
            "loss: 0.212897  [ 6464/60000]\n",
            "loss: 0.039226  [12864/60000]\n",
            "loss: 0.069122  [19264/60000]\n",
            "loss: 0.142434  [25664/60000]\n",
            "loss: 0.174447  [32064/60000]\n",
            "loss: 0.080385  [38464/60000]\n",
            "loss: 0.083403  [44864/60000]\n",
            "loss: 0.123659  [51264/60000]\n",
            "loss: 0.130000  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.532343 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.078962  [   64/60000]\n",
            "loss: 0.204190  [ 6464/60000]\n",
            "loss: 0.070673  [12864/60000]\n",
            "loss: 0.055911  [19264/60000]\n",
            "loss: 0.148004  [25664/60000]\n",
            "loss: 0.162533  [32064/60000]\n",
            "loss: 0.076917  [38464/60000]\n",
            "loss: 0.106321  [44864/60000]\n",
            "loss: 0.125435  [51264/60000]\n",
            "loss: 0.130330  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.510865 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.075043  [   64/60000]\n",
            "loss: 0.220860  [ 6464/60000]\n",
            "loss: 0.039109  [12864/60000]\n",
            "loss: 0.110887  [19264/60000]\n",
            "loss: 0.192629  [25664/60000]\n",
            "loss: 0.154944  [32064/60000]\n",
            "loss: 0.070341  [38464/60000]\n",
            "loss: 0.104754  [44864/60000]\n",
            "loss: 0.114337  [51264/60000]\n",
            "loss: 0.100207  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.549156 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.054570  [   64/60000]\n",
            "loss: 0.174212  [ 6464/60000]\n",
            "loss: 0.056051  [12864/60000]\n",
            "loss: 0.076958  [19264/60000]\n",
            "loss: 0.142672  [25664/60000]\n",
            "loss: 0.136472  [32064/60000]\n",
            "loss: 0.075062  [38464/60000]\n",
            "loss: 0.086262  [44864/60000]\n",
            "loss: 0.135199  [51264/60000]\n",
            "loss: 0.132553  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.559467 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.076159  [   64/60000]\n",
            "loss: 0.173645  [ 6464/60000]\n",
            "loss: 0.054339  [12864/60000]\n",
            "loss: 0.076093  [19264/60000]\n",
            "loss: 0.109376  [25664/60000]\n",
            "loss: 0.130794  [32064/60000]\n",
            "loss: 0.104204  [38464/60000]\n",
            "loss: 0.109562  [44864/60000]\n",
            "loss: 0.107872  [51264/60000]\n",
            "loss: 0.120206  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.522374 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.072508  [   64/60000]\n",
            "loss: 0.196751  [ 6464/60000]\n",
            "loss: 0.028682  [12864/60000]\n",
            "loss: 0.093035  [19264/60000]\n",
            "loss: 0.140743  [25664/60000]\n",
            "loss: 0.132278  [32064/60000]\n",
            "loss: 0.064423  [38464/60000]\n",
            "loss: 0.094101  [44864/60000]\n",
            "loss: 0.098953  [51264/60000]\n",
            "loss: 0.118643  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.547203 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.061963  [   64/60000]\n",
            "loss: 0.206072  [ 6464/60000]\n",
            "loss: 0.037385  [12864/60000]\n",
            "loss: 0.082882  [19264/60000]\n",
            "loss: 0.195962  [25664/60000]\n",
            "loss: 0.123421  [32064/60000]\n",
            "loss: 0.116567  [38464/60000]\n",
            "loss: 0.096820  [44864/60000]\n",
            "loss: 0.113401  [51264/60000]\n",
            "loss: 0.118209  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.575270 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.078611  [   64/60000]\n",
            "loss: 0.171999  [ 6464/60000]\n",
            "loss: 0.038192  [12864/60000]\n",
            "loss: 0.082288  [19264/60000]\n",
            "loss: 0.133193  [25664/60000]\n",
            "loss: 0.158693  [32064/60000]\n",
            "loss: 0.064433  [38464/60000]\n",
            "loss: 0.127900  [44864/60000]\n",
            "loss: 0.086371  [51264/60000]\n",
            "loss: 0.181549  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.9%, Avg loss: 0.589004 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.103415  [   64/60000]\n",
            "loss: 0.205484  [ 6464/60000]\n",
            "loss: 0.039472  [12864/60000]\n",
            "loss: 0.075720  [19264/60000]\n",
            "loss: 0.126705  [25664/60000]\n",
            "loss: 0.223229  [32064/60000]\n",
            "loss: 0.069967  [38464/60000]\n",
            "loss: 0.122609  [44864/60000]\n",
            "loss: 0.119666  [51264/60000]\n",
            "loss: 0.137454  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.566209 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.075885  [   64/60000]\n",
            "loss: 0.190230  [ 6464/60000]\n",
            "loss: 0.038222  [12864/60000]\n",
            "loss: 0.077061  [19264/60000]\n",
            "loss: 0.208760  [25664/60000]\n",
            "loss: 0.131598  [32064/60000]\n",
            "loss: 0.055591  [38464/60000]\n",
            "loss: 0.085653  [44864/60000]\n",
            "loss: 0.145519  [51264/60000]\n",
            "loss: 0.068461  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.551457 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.074791  [   64/60000]\n",
            "loss: 0.210577  [ 6464/60000]\n",
            "loss: 0.049127  [12864/60000]\n",
            "loss: 0.159095  [19264/60000]\n",
            "loss: 0.153108  [25664/60000]\n",
            "loss: 0.159304  [32064/60000]\n",
            "loss: 0.045678  [38464/60000]\n",
            "loss: 0.099088  [44864/60000]\n",
            "loss: 0.136562  [51264/60000]\n",
            "loss: 0.111353  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.606496 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.072812  [   64/60000]\n",
            "loss: 0.147686  [ 6464/60000]\n",
            "loss: 0.043008  [12864/60000]\n",
            "loss: 0.040690  [19264/60000]\n",
            "loss: 0.114075  [25664/60000]\n",
            "loss: 0.095660  [32064/60000]\n",
            "loss: 0.093083  [38464/60000]\n",
            "loss: 0.064912  [44864/60000]\n",
            "loss: 0.119646  [51264/60000]\n",
            "loss: 0.101729  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.582966 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.055593  [   64/60000]\n",
            "loss: 0.147961  [ 6464/60000]\n",
            "loss: 0.039145  [12864/60000]\n",
            "loss: 0.083118  [19264/60000]\n",
            "loss: 0.235696  [25664/60000]\n",
            "loss: 0.197893  [32064/60000]\n",
            "loss: 0.038998  [38464/60000]\n",
            "loss: 0.110126  [44864/60000]\n",
            "loss: 0.099490  [51264/60000]\n",
            "loss: 0.118659  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.586734 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.091040  [   64/60000]\n",
            "loss: 0.164194  [ 6464/60000]\n",
            "loss: 0.041270  [12864/60000]\n",
            "loss: 0.063145  [19264/60000]\n",
            "loss: 0.111983  [25664/60000]\n",
            "loss: 0.114349  [32064/60000]\n",
            "loss: 0.061496  [38464/60000]\n",
            "loss: 0.080306  [44864/60000]\n",
            "loss: 0.105596  [51264/60000]\n",
            "loss: 0.086114  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.616046 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.065180  [   64/60000]\n",
            "loss: 0.208562  [ 6464/60000]\n",
            "loss: 0.034590  [12864/60000]\n",
            "loss: 0.072646  [19264/60000]\n",
            "loss: 0.089511  [25664/60000]\n",
            "loss: 0.140600  [32064/60000]\n",
            "loss: 0.044267  [38464/60000]\n",
            "loss: 0.075197  [44864/60000]\n",
            "loss: 0.111478  [51264/60000]\n",
            "loss: 0.052674  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.605506 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.068172  [   64/60000]\n",
            "loss: 0.119978  [ 6464/60000]\n",
            "loss: 0.032097  [12864/60000]\n",
            "loss: 0.046250  [19264/60000]\n",
            "loss: 0.103962  [25664/60000]\n",
            "loss: 0.125238  [32064/60000]\n",
            "loss: 0.095635  [38464/60000]\n",
            "loss: 0.075937  [44864/60000]\n",
            "loss: 0.106443  [51264/60000]\n",
            "loss: 0.080981  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.584995 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.047648  [   64/60000]\n",
            "loss: 0.191922  [ 6464/60000]\n",
            "loss: 0.023466  [12864/60000]\n",
            "loss: 0.041599  [19264/60000]\n",
            "loss: 0.139442  [25664/60000]\n",
            "loss: 0.079740  [32064/60000]\n",
            "loss: 0.056922  [38464/60000]\n",
            "loss: 0.098987  [44864/60000]\n",
            "loss: 0.139006  [51264/60000]\n",
            "loss: 0.061369  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.609795 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.067307  [   64/60000]\n",
            "loss: 0.133751  [ 6464/60000]\n",
            "loss: 0.027670  [12864/60000]\n",
            "loss: 0.039986  [19264/60000]\n",
            "loss: 0.181241  [25664/60000]\n",
            "loss: 0.154490  [32064/60000]\n",
            "loss: 0.056313  [38464/60000]\n",
            "loss: 0.105033  [44864/60000]\n",
            "loss: 0.075436  [51264/60000]\n",
            "loss: 0.098211  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.633400 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.079999  [   64/60000]\n",
            "loss: 0.176656  [ 6464/60000]\n",
            "loss: 0.039315  [12864/60000]\n",
            "loss: 0.053241  [19264/60000]\n",
            "loss: 0.083036  [25664/60000]\n",
            "loss: 0.153131  [32064/60000]\n",
            "loss: 0.079598  [38464/60000]\n",
            "loss: 0.074643  [44864/60000]\n",
            "loss: 0.067823  [51264/60000]\n",
            "loss: 0.059357  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.647573 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.107597  [   64/60000]\n",
            "loss: 0.113152  [ 6464/60000]\n",
            "loss: 0.052689  [12864/60000]\n",
            "loss: 0.062229  [19264/60000]\n",
            "loss: 0.124229  [25664/60000]\n",
            "loss: 0.132126  [32064/60000]\n",
            "loss: 0.079272  [38464/60000]\n",
            "loss: 0.094073  [44864/60000]\n",
            "loss: 0.080124  [51264/60000]\n",
            "loss: 0.116283  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.627701 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.061780  [   64/60000]\n",
            "loss: 0.178797  [ 6464/60000]\n",
            "loss: 0.038562  [12864/60000]\n",
            "loss: 0.055518  [19264/60000]\n",
            "loss: 0.163207  [25664/60000]\n",
            "loss: 0.094851  [32064/60000]\n",
            "loss: 0.056521  [38464/60000]\n",
            "loss: 0.076522  [44864/60000]\n",
            "loss: 0.087971  [51264/60000]\n",
            "loss: 0.075757  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.644745 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.054505  [   64/60000]\n",
            "loss: 0.185527  [ 6464/60000]\n",
            "loss: 0.023871  [12864/60000]\n",
            "loss: 0.116812  [19264/60000]\n",
            "loss: 0.128240  [25664/60000]\n",
            "loss: 0.146637  [32064/60000]\n",
            "loss: 0.047578  [38464/60000]\n",
            "loss: 0.106176  [44864/60000]\n",
            "loss: 0.099679  [51264/60000]\n",
            "loss: 0.058650  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.641185 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.084993  [   64/60000]\n",
            "loss: 0.138711  [ 6464/60000]\n",
            "loss: 0.016237  [12864/60000]\n",
            "loss: 0.059974  [19264/60000]\n",
            "loss: 0.125435  [25664/60000]\n",
            "loss: 0.133332  [32064/60000]\n",
            "loss: 0.082395  [38464/60000]\n",
            "loss: 0.066914  [44864/60000]\n",
            "loss: 0.133041  [51264/60000]\n",
            "loss: 0.049952  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.654436 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.037053  [   64/60000]\n",
            "loss: 0.133505  [ 6464/60000]\n",
            "loss: 0.080191  [12864/60000]\n",
            "loss: 0.062078  [19264/60000]\n",
            "loss: 0.175441  [25664/60000]\n",
            "loss: 0.067558  [32064/60000]\n",
            "loss: 0.132477  [38464/60000]\n",
            "loss: 0.080161  [44864/60000]\n",
            "loss: 0.087411  [51264/60000]\n",
            "loss: 0.033729  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.716851 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.062801  [   64/60000]\n",
            "loss: 0.139632  [ 6464/60000]\n",
            "loss: 0.027802  [12864/60000]\n",
            "loss: 0.076150  [19264/60000]\n",
            "loss: 0.141013  [25664/60000]\n",
            "loss: 0.161502  [32064/60000]\n",
            "loss: 0.041487  [38464/60000]\n",
            "loss: 0.073059  [44864/60000]\n",
            "loss: 0.102550  [51264/60000]\n",
            "loss: 0.046451  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.656252 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.052611  [   64/60000]\n",
            "loss: 0.137402  [ 6464/60000]\n",
            "loss: 0.024612  [12864/60000]\n",
            "loss: 0.042054  [19264/60000]\n",
            "loss: 0.098580  [25664/60000]\n",
            "loss: 0.099607  [32064/60000]\n",
            "loss: 0.047546  [38464/60000]\n",
            "loss: 0.061523  [44864/60000]\n",
            "loss: 0.059372  [51264/60000]\n",
            "loss: 0.064390  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.673472 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.034062  [   64/60000]\n",
            "loss: 0.086324  [ 6464/60000]\n",
            "loss: 0.028632  [12864/60000]\n",
            "loss: 0.053246  [19264/60000]\n",
            "loss: 0.142608  [25664/60000]\n",
            "loss: 0.052953  [32064/60000]\n",
            "loss: 0.047649  [38464/60000]\n",
            "loss: 0.063953  [44864/60000]\n",
            "loss: 0.076918  [51264/60000]\n",
            "loss: 0.036719  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.660674 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.064764  [   64/60000]\n",
            "loss: 0.153079  [ 6464/60000]\n",
            "loss: 0.046527  [12864/60000]\n",
            "loss: 0.055720  [19264/60000]\n",
            "loss: 0.111214  [25664/60000]\n",
            "loss: 0.231085  [32064/60000]\n",
            "loss: 0.057020  [38464/60000]\n",
            "loss: 0.060190  [44864/60000]\n",
            "loss: 0.085441  [51264/60000]\n",
            "loss: 0.040703  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.636062 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.062728  [   64/60000]\n",
            "loss: 0.100918  [ 6464/60000]\n",
            "loss: 0.038992  [12864/60000]\n",
            "loss: 0.031055  [19264/60000]\n",
            "loss: 0.190457  [25664/60000]\n",
            "loss: 0.165635  [32064/60000]\n",
            "loss: 0.046624  [38464/60000]\n",
            "loss: 0.047226  [44864/60000]\n",
            "loss: 0.067978  [51264/60000]\n",
            "loss: 0.044324  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.684380 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.032460  [   64/60000]\n",
            "loss: 0.161148  [ 6464/60000]\n",
            "loss: 0.043751  [12864/60000]\n",
            "loss: 0.064090  [19264/60000]\n",
            "loss: 0.080926  [25664/60000]\n",
            "loss: 0.146148  [32064/60000]\n",
            "loss: 0.034804  [38464/60000]\n",
            "loss: 0.107671  [44864/60000]\n",
            "loss: 0.121321  [51264/60000]\n",
            "loss: 0.053716  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.694688 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.115687  [   64/60000]\n",
            "loss: 0.144423  [ 6464/60000]\n",
            "loss: 0.033736  [12864/60000]\n",
            "loss: 0.053410  [19264/60000]\n",
            "loss: 0.132802  [25664/60000]\n",
            "loss: 0.113171  [32064/60000]\n",
            "loss: 0.017864  [38464/60000]\n",
            "loss: 0.074181  [44864/60000]\n",
            "loss: 0.096257  [51264/60000]\n",
            "loss: 0.088180  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.698335 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.041111  [   64/60000]\n",
            "loss: 0.173259  [ 6464/60000]\n",
            "loss: 0.083564  [12864/60000]\n",
            "loss: 0.088477  [19264/60000]\n",
            "loss: 0.149150  [25664/60000]\n",
            "loss: 0.121968  [32064/60000]\n",
            "loss: 0.037840  [38464/60000]\n",
            "loss: 0.075181  [44864/60000]\n",
            "loss: 0.210827  [51264/60000]\n",
            "loss: 0.034390  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.650324 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.046425  [   64/60000]\n",
            "loss: 0.086140  [ 6464/60000]\n",
            "loss: 0.012389  [12864/60000]\n",
            "loss: 0.041593  [19264/60000]\n",
            "loss: 0.122741  [25664/60000]\n",
            "loss: 0.147596  [32064/60000]\n",
            "loss: 0.027601  [38464/60000]\n",
            "loss: 0.071974  [44864/60000]\n",
            "loss: 0.051562  [51264/60000]\n",
            "loss: 0.073558  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.677927 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.038326  [   64/60000]\n",
            "loss: 0.152867  [ 6464/60000]\n",
            "loss: 0.034628  [12864/60000]\n",
            "loss: 0.065674  [19264/60000]\n",
            "loss: 0.073443  [25664/60000]\n",
            "loss: 0.139198  [32064/60000]\n",
            "loss: 0.031006  [38464/60000]\n",
            "loss: 0.067999  [44864/60000]\n",
            "loss: 0.075853  [51264/60000]\n",
            "loss: 0.039638  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.693705 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.057655  [   64/60000]\n",
            "loss: 0.081668  [ 6464/60000]\n",
            "loss: 0.046028  [12864/60000]\n",
            "loss: 0.051514  [19264/60000]\n",
            "loss: 0.105697  [25664/60000]\n",
            "loss: 0.161715  [32064/60000]\n",
            "loss: 0.058615  [38464/60000]\n",
            "loss: 0.100910  [44864/60000]\n",
            "loss: 0.059614  [51264/60000]\n",
            "loss: 0.055722  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.9%, Avg loss: 0.698340 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.044356  [   64/60000]\n",
            "loss: 0.110596  [ 6464/60000]\n",
            "loss: 0.051854  [12864/60000]\n",
            "loss: 0.069981  [19264/60000]\n",
            "loss: 0.122859  [25664/60000]\n",
            "loss: 0.194454  [32064/60000]\n",
            "loss: 0.064953  [38464/60000]\n",
            "loss: 0.051229  [44864/60000]\n",
            "loss: 0.076947  [51264/60000]\n",
            "loss: 0.026090  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.684475 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.022955  [   64/60000]\n",
            "loss: 0.080469  [ 6464/60000]\n",
            "loss: 0.016423  [12864/60000]\n",
            "loss: 0.042739  [19264/60000]\n",
            "loss: 0.153288  [25664/60000]\n",
            "loss: 0.142808  [32064/60000]\n",
            "loss: 0.030222  [38464/60000]\n",
            "loss: 0.080412  [44864/60000]\n",
            "loss: 0.041327  [51264/60000]\n",
            "loss: 0.038818  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.8%, Avg loss: 0.679952 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.052286  [   64/60000]\n",
            "loss: 0.132284  [ 6464/60000]\n",
            "loss: 0.012618  [12864/60000]\n",
            "loss: 0.041241  [19264/60000]\n",
            "loss: 0.094891  [25664/60000]\n",
            "loss: 0.134116  [32064/60000]\n",
            "loss: 0.057526  [38464/60000]\n",
            "loss: 0.045453  [44864/60000]\n",
            "loss: 0.078119  [51264/60000]\n",
            "loss: 0.131704  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.787072 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.095349  [   64/60000]\n",
            "loss: 0.083098  [ 6464/60000]\n",
            "loss: 0.026621  [12864/60000]\n",
            "loss: 0.071521  [19264/60000]\n",
            "loss: 0.062681  [25664/60000]\n",
            "loss: 0.110917  [32064/60000]\n",
            "loss: 0.051391  [38464/60000]\n",
            "loss: 0.060014  [44864/60000]\n",
            "loss: 0.046930  [51264/60000]\n",
            "loss: 0.024850  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.710328 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.061553  [   64/60000]\n",
            "loss: 0.028214  [ 6464/60000]\n",
            "loss: 0.015396  [12864/60000]\n",
            "loss: 0.050698  [19264/60000]\n",
            "loss: 0.041272  [25664/60000]\n",
            "loss: 0.120215  [32064/60000]\n",
            "loss: 0.035622  [38464/60000]\n",
            "loss: 0.068929  [44864/60000]\n",
            "loss: 0.065849  [51264/60000]\n",
            "loss: 0.033457  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.2%, Avg loss: 0.745561 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.057722  [   64/60000]\n",
            "loss: 0.083222  [ 6464/60000]\n",
            "loss: 0.024968  [12864/60000]\n",
            "loss: 0.091365  [19264/60000]\n",
            "loss: 0.053922  [25664/60000]\n",
            "loss: 0.149558  [32064/60000]\n",
            "loss: 0.177385  [38464/60000]\n",
            "loss: 0.103110  [44864/60000]\n",
            "loss: 0.080639  [51264/60000]\n",
            "loss: 0.062749  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.715646 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.043492  [   64/60000]\n",
            "loss: 0.079561  [ 6464/60000]\n",
            "loss: 0.042406  [12864/60000]\n",
            "loss: 0.098595  [19264/60000]\n",
            "loss: 0.066286  [25664/60000]\n",
            "loss: 0.120841  [32064/60000]\n",
            "loss: 0.070577  [38464/60000]\n",
            "loss: 0.032355  [44864/60000]\n",
            "loss: 0.061511  [51264/60000]\n",
            "loss: 0.025876  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.748939 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, lossFn, optimizer1)\n",
        "    test_loop(test_dataloader, model, lossFn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb-wX4qXV4dA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkh8J5uaT2THwSej11kTtH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}