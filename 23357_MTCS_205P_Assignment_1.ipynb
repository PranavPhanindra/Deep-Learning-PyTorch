{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PranavPhanindra/Deep-Learning-PyTorch/blob/main/23357_MTCS_205P_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P8WcNwtbgfM"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci4sQkUSOZdO",
        "outputId": "ba916e03-facb-497c-f890-9b1b0cbc3f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.1.0+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=7674d0b2806406d4bb1aa2a7b3c3c7f85aec3bb692557010283f15123fde1d27\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz\n",
        "\n",
        "!pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G7gdBlqfv_-p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchviz import make_dot\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gsygWWJebsq"
      },
      "source": [
        "# Data Loading and info about Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtS5JpmwegFB",
        "outputId": "ab0c1c13-8e0a-4b06-be2f-c87df4bb439a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17663612.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 300633.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5555134.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5504021.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6QZxzgnViBA0"
      },
      "outputs": [],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "\n",
        "#Defining labels for different classes of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "O3t2B97Se-1i",
        "outputId": "b215cda4-b423-4946-c313-a0c8f5300d39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "img,label = training_data[1]\n",
        "img = img.reshape(28,28)\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.title(labels_map[label])\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQmu2EJmy3H3",
        "outputId": "bccced8b-adfc-448c-9c73-be4423e7a7e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.1608, 0.7373, 0.4039, 0.2118, 0.1882, 0.1686, 0.3412, 0.6588,\n",
              "         0.5216, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.1922, 0.5333,\n",
              "         0.8588, 0.8471, 0.8941, 0.9255, 1.0000, 1.0000, 1.0000, 1.0000, 0.8510,\n",
              "         0.8431, 0.9961, 0.9059, 0.6275, 0.1765, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.6902, 0.8706, 0.8784,\n",
              "         0.8314, 0.7961, 0.7765, 0.7686, 0.7843, 0.8431, 0.8000, 0.7922, 0.7882,\n",
              "         0.7882, 0.7882, 0.8196, 0.8549, 0.8784, 0.6431, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 0.8588, 0.7843, 0.7765,\n",
              "         0.7922, 0.7765, 0.7804, 0.7804, 0.7882, 0.7686, 0.7765, 0.7765, 0.7843,\n",
              "         0.7843, 0.7843, 0.7843, 0.7882, 0.7843, 0.8824, 0.1608, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.8588, 0.7804, 0.7961, 0.7961,\n",
              "         0.8314, 0.9333, 0.9725, 0.9804, 0.9608, 0.9765, 0.9647, 0.9686, 0.9882,\n",
              "         0.9725, 0.9216, 0.8118, 0.7961, 0.7961, 0.8706, 0.5490, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4549, 0.8863, 0.8078, 0.8000, 0.8118,\n",
              "         0.8000, 0.3961, 0.2941, 0.1843, 0.2863, 0.1882, 0.1961, 0.1765, 0.2000,\n",
              "         0.2471, 0.4431, 0.8706, 0.7922, 0.8078, 0.8627, 0.8784, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7843, 0.8706, 0.8196, 0.7961, 0.8431,\n",
              "         0.7843, 0.0000, 0.2745, 0.3843, 0.0000, 0.4039, 0.2314, 0.2667, 0.2784,\n",
              "         0.1922, 0.0000, 0.8588, 0.8078, 0.8392, 0.8235, 0.9804, 0.1490, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9686, 0.8549, 0.8314, 0.8235, 0.8431,\n",
              "         0.8392, 0.0000, 0.9961, 0.9529, 0.5451, 1.0000, 0.6824, 0.9843, 1.0000,\n",
              "         0.8039, 0.0000, 0.8431, 0.8510, 0.8392, 0.8157, 0.8627, 0.3725, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.1765, 0.8863, 0.8392, 0.8392, 0.8431, 0.8784,\n",
              "         0.8039, 0.0000, 0.1647, 0.1373, 0.2353, 0.0627, 0.0667, 0.0471, 0.0510,\n",
              "         0.2745, 0.0000, 0.7412, 0.8471, 0.8314, 0.8078, 0.8314, 0.6118, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.6431, 0.9216, 0.8392, 0.8275, 0.8627, 0.8471,\n",
              "         0.7882, 0.2039, 0.2784, 0.3490, 0.3686, 0.3255, 0.3059, 0.2745, 0.2980,\n",
              "         0.3608, 0.3412, 0.8078, 0.8118, 0.8706, 0.8353, 0.8588, 0.8157, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.4157, 0.7333, 0.8745, 0.9294, 0.9725, 0.8275,\n",
              "         0.7765, 0.9882, 0.9804, 0.9725, 0.9608, 0.9725, 0.9882, 0.9922, 0.9804,\n",
              "         0.9882, 0.9373, 0.7882, 0.8314, 0.8824, 0.8431, 0.7569, 0.4431, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.2118, 0.6235, 0.8706,\n",
              "         0.7569, 0.8157, 0.7529, 0.7725, 0.7843, 0.7843, 0.7843, 0.7843, 0.7882,\n",
              "         0.7961, 0.7647, 0.8235, 0.6471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1843, 0.8824,\n",
              "         0.7529, 0.8392, 0.7961, 0.8078, 0.8000, 0.8000, 0.8039, 0.8078, 0.8000,\n",
              "         0.8314, 0.7725, 0.8549, 0.4196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0235, 0.0000, 0.1804, 0.8314,\n",
              "         0.7647, 0.8314, 0.7922, 0.8078, 0.8039, 0.8000, 0.8039, 0.8078, 0.8000,\n",
              "         0.8314, 0.7843, 0.8549, 0.3569, 0.0000, 0.0118, 0.0039, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0431, 0.7725,\n",
              "         0.7804, 0.8039, 0.7922, 0.8039, 0.8078, 0.8000, 0.8039, 0.8118, 0.8000,\n",
              "         0.8039, 0.8039, 0.8549, 0.3020, 0.0000, 0.0196, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.0078, 0.7490,\n",
              "         0.7765, 0.7882, 0.8039, 0.8078, 0.8039, 0.8039, 0.8078, 0.8196, 0.8078,\n",
              "         0.7804, 0.8196, 0.8588, 0.2902, 0.0000, 0.0196, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.7373,\n",
              "         0.7725, 0.7843, 0.8118, 0.8118, 0.8000, 0.8118, 0.8118, 0.8235, 0.8157,\n",
              "         0.7765, 0.8118, 0.8667, 0.2824, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.8431,\n",
              "         0.7765, 0.7961, 0.8078, 0.8157, 0.8039, 0.8118, 0.8118, 0.8235, 0.8157,\n",
              "         0.7843, 0.7922, 0.8706, 0.2941, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.8314,\n",
              "         0.7765, 0.8196, 0.8078, 0.8196, 0.8078, 0.8157, 0.8118, 0.8275, 0.8078,\n",
              "         0.8039, 0.7765, 0.8667, 0.3137, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.8000,\n",
              "         0.7882, 0.8039, 0.8157, 0.8118, 0.8039, 0.8275, 0.8039, 0.8235, 0.8235,\n",
              "         0.8196, 0.7647, 0.8667, 0.3765, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.7922,\n",
              "         0.7882, 0.8039, 0.8196, 0.8118, 0.8039, 0.8353, 0.8078, 0.8235, 0.8196,\n",
              "         0.8235, 0.7608, 0.8510, 0.4118, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.8000,\n",
              "         0.8000, 0.8039, 0.8157, 0.8118, 0.8039, 0.8431, 0.8118, 0.8235, 0.8157,\n",
              "         0.8275, 0.7569, 0.8353, 0.4510, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8000,\n",
              "         0.8118, 0.8118, 0.8157, 0.8078, 0.8078, 0.8431, 0.8235, 0.8235, 0.8118,\n",
              "         0.8314, 0.7647, 0.8235, 0.4627, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.7765,\n",
              "         0.8157, 0.8157, 0.8157, 0.8000, 0.8118, 0.8314, 0.8314, 0.8235, 0.8118,\n",
              "         0.8275, 0.7686, 0.8118, 0.4745, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.7765,\n",
              "         0.8235, 0.8118, 0.8157, 0.8078, 0.8196, 0.8353, 0.8314, 0.8275, 0.8118,\n",
              "         0.8235, 0.7725, 0.8118, 0.4863, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6745,\n",
              "         0.8235, 0.7961, 0.7882, 0.7804, 0.8000, 0.8118, 0.8039, 0.8000, 0.7882,\n",
              "         0.8039, 0.7725, 0.8078, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373,\n",
              "         0.8667, 0.8392, 0.9176, 0.9255, 0.9333, 0.9569, 0.9569, 0.9569, 0.9412,\n",
              "         0.9529, 0.8392, 0.8784, 0.6353, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.5451,\n",
              "         0.5725, 0.5098, 0.5294, 0.5294, 0.5373, 0.4902, 0.4863, 0.4902, 0.4745,\n",
              "         0.4667, 0.4471, 0.5098, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orhS4OKbbxb0"
      },
      "source": [
        "# Creating Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Aun1NS_Hb1R9"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            #Layer 1 - 64 Nodes\n",
        "            #Layer 2 - 128 Nodes\n",
        "            #Relu Activation for each layer\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64 , 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10),\n",
        "        )\n",
        "#Can also declare the layers as linear layer 1 etc..\n",
        "#defining forward function that passes the data on each epoch\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY_U_qwmdxwK"
      },
      "source": [
        "Checking for the devices we have so that we can run the code either on GPU or on CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm4teak5dscY",
        "outputId": "ffab0e30-e249-4c9d-d93b-00103feaae60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2VioxzZb8cR",
        "outputId": "a238602c-7fff-48bf-e860-9bf584735633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkl7NW2uMVUd"
      },
      "source": [
        "Here we see how the initialisations are done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVfZp4GYHjyP",
        "outputId": "d3262976-ca05-4f11-9233-97791a343bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------\n",
            "('0.weight', Parameter containing:\n",
            "tensor([[-0.0162, -0.0219,  0.0166,  ..., -0.0118,  0.0351, -0.0091],\n",
            "        [ 0.0057,  0.0050, -0.0163,  ...,  0.0079, -0.0060, -0.0284],\n",
            "        [ 0.0030,  0.0154, -0.0297,  ...,  0.0140, -0.0353, -0.0254],\n",
            "        ...,\n",
            "        [ 0.0009, -0.0074,  0.0237,  ...,  0.0111, -0.0115,  0.0229],\n",
            "        [-0.0171, -0.0111,  0.0035,  ..., -0.0116, -0.0251, -0.0315],\n",
            "        [-0.0274, -0.0261,  0.0327,  ...,  0.0294,  0.0335, -0.0020]],\n",
            "       requires_grad=True))\n",
            "torch.Size([64, 784])\n",
            "---------------------------------------------------------------------------------------\n",
            "('0.bias', Parameter containing:\n",
            "tensor([-0.0164, -0.0278,  0.0322,  0.0080,  0.0233, -0.0087,  0.0188, -0.0197,\n",
            "         0.0179, -0.0340,  0.0283, -0.0029, -0.0040,  0.0304, -0.0178, -0.0162,\n",
            "         0.0216, -0.0061, -0.0319, -0.0197,  0.0336, -0.0202,  0.0109,  0.0149,\n",
            "         0.0195, -0.0090,  0.0185, -0.0116,  0.0071, -0.0206,  0.0128,  0.0262,\n",
            "        -0.0084, -0.0198,  0.0294, -0.0006,  0.0232, -0.0002, -0.0170, -0.0298,\n",
            "        -0.0319,  0.0209, -0.0064,  0.0189,  0.0002, -0.0262,  0.0321,  0.0100,\n",
            "         0.0211,  0.0174,  0.0003, -0.0298, -0.0273, -0.0339,  0.0121,  0.0001,\n",
            "        -0.0224, -0.0037, -0.0278,  0.0144,  0.0216, -0.0211, -0.0322, -0.0035],\n",
            "       requires_grad=True))\n",
            "torch.Size([64])\n",
            "---------------------------------------------------------------------------------------\n",
            "('2.weight', Parameter containing:\n",
            "tensor([[ 0.1218,  0.0570,  0.0502,  ...,  0.0801,  0.0296,  0.0779],\n",
            "        [-0.1027,  0.0098, -0.0822,  ..., -0.0889,  0.0136,  0.0402],\n",
            "        [ 0.0450,  0.0181, -0.0706,  ..., -0.0056,  0.0396,  0.0982],\n",
            "        ...,\n",
            "        [ 0.0495,  0.0339, -0.1088,  ...,  0.0847, -0.1208, -0.0488],\n",
            "        [-0.0258,  0.0262, -0.0487,  ...,  0.0018, -0.0864,  0.0062],\n",
            "        [ 0.0433,  0.0709, -0.0893,  ...,  0.0017,  0.0989, -0.0428]],\n",
            "       requires_grad=True))\n",
            "torch.Size([128, 64])\n",
            "---------------------------------------------------------------------------------------\n",
            "('2.bias', Parameter containing:\n",
            "tensor([-0.0540,  0.0907,  0.0195, -0.0613,  0.1221, -0.0893,  0.1248,  0.1235,\n",
            "        -0.0428, -0.0391, -0.0333, -0.1070,  0.0503, -0.0936,  0.0181, -0.0182,\n",
            "        -0.0178, -0.1162, -0.0981, -0.0909, -0.0541,  0.1151,  0.0731, -0.0642,\n",
            "         0.0903, -0.0009, -0.0517, -0.0625, -0.1206, -0.0516,  0.0904, -0.1119,\n",
            "        -0.0397,  0.1036,  0.0581, -0.0922,  0.1229, -0.1148, -0.0837, -0.1241,\n",
            "        -0.0994, -0.0555, -0.0098, -0.0285, -0.0136, -0.0120,  0.1144, -0.0104,\n",
            "         0.0715, -0.0545,  0.0503, -0.0398,  0.0917,  0.0422,  0.0752, -0.1241,\n",
            "         0.0281, -0.0633, -0.0046, -0.1071,  0.0505, -0.0995, -0.0193,  0.0480,\n",
            "         0.0499, -0.1212, -0.0994,  0.0223, -0.0281,  0.0779,  0.1030, -0.0460,\n",
            "         0.0228, -0.0367,  0.0737, -0.0917, -0.0372, -0.0836, -0.0874,  0.1225,\n",
            "        -0.0380, -0.0885, -0.0994, -0.0182,  0.1245,  0.0394,  0.1190, -0.0348,\n",
            "         0.0420,  0.0439,  0.0574, -0.0804, -0.1188,  0.0554, -0.0231,  0.0424,\n",
            "        -0.0630, -0.1047, -0.0265, -0.0009, -0.0556,  0.0281, -0.0229,  0.0084,\n",
            "        -0.0649,  0.0144,  0.0639,  0.0004,  0.0430, -0.1210, -0.0721, -0.0068,\n",
            "         0.0950, -0.0591,  0.0452, -0.0799, -0.0316, -0.0192,  0.0394, -0.0230,\n",
            "         0.0060,  0.0197, -0.0303, -0.0938, -0.1140, -0.0624,  0.0332, -0.0296],\n",
            "       requires_grad=True))\n",
            "torch.Size([128])\n",
            "---------------------------------------------------------------------------------------\n",
            "('4.weight', Parameter containing:\n",
            "tensor([[-0.0033, -0.0153, -0.0391,  ..., -0.0098, -0.0837, -0.0690],\n",
            "        [-0.0532, -0.0053, -0.0514,  ..., -0.0705, -0.0628,  0.0710],\n",
            "        [-0.0363,  0.0057,  0.0630,  ..., -0.0039,  0.0357, -0.0782],\n",
            "        ...,\n",
            "        [-0.0316,  0.0596, -0.0813,  ...,  0.0708,  0.0229,  0.0880],\n",
            "        [-0.0325,  0.0256,  0.0619,  ...,  0.0584,  0.0269, -0.0128],\n",
            "        [ 0.0476, -0.0433, -0.0485,  ..., -0.0770, -0.0365, -0.0780]],\n",
            "       requires_grad=True))\n",
            "torch.Size([10, 128])\n",
            "---------------------------------------------------------------------------------------\n",
            "('4.bias', Parameter containing:\n",
            "tensor([-0.0561, -0.0251,  0.0692, -0.0537, -0.0829,  0.0725,  0.0121,  0.0327,\n",
            "        -0.0705, -0.0430], requires_grad=True))\n",
            "torch.Size([10])\n",
            "---------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\"\"\"for params in model.parameters() :\n",
        "  print(params)\"\"\"\n",
        "print(\"---------------------------------------------------------------------------------------\")\n",
        "for linParams in model.linear_relu_stack.named_parameters():\n",
        "  print(linParams)\n",
        "  print(linParams[1].size())\n",
        "  print(\"---------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtnLqwdxqCmm"
      },
      "source": [
        "Hyper Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr9MIpmaD-NT"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0pAoa6DOD8k6"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader,model,lossFn,optimizer1) :\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  for batch,(X,y) in enumerate(dataloader) :\n",
        "\n",
        "    #Predictions and loss as we call forward and loss is calculated to be further used\n",
        "    pred = model(X)\n",
        "    #make_dot(pred, params=dict(model.named_parameters()))\n",
        "    loss = lossFn(pred,y)\n",
        "\n",
        "    #Backpropagation\n",
        "    #Calculation of Gradient\n",
        "    loss.backward()\n",
        "    #This would update the weights and biases\n",
        "    optimizer1.step()\n",
        "    #This would zero down the gradients so that they arent added up in next step\n",
        "    optimizer1.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u__E7Qo_U4YS"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    #Set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "COETMezjeWOL"
      },
      "outputs": [],
      "source": [
        "learningRate = 1e-1\n",
        "batchSize  = 128\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LpRbqE71lXBl"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=batchSize)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batchSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p6WB1SJ0h2eH"
      },
      "outputs": [],
      "source": [
        "#Combines LogSoftmax and NLLLoss - Negativce log likelihood\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "optimizer1 = torch.optim.SGD(model.parameters(),lr = learningRate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtPnVfzsVubJ",
        "outputId": "48723510-0b23-45df-bee6-fc7b2cb96d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.226266  [  128/60000]\n",
            "loss: 0.225986  [12928/60000]\n",
            "loss: 0.269020  [25728/60000]\n",
            "loss: 0.279151  [38528/60000]\n",
            "loss: 0.323790  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.358314 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.181390  [  128/60000]\n",
            "loss: 0.223912  [12928/60000]\n",
            "loss: 0.265356  [25728/60000]\n",
            "loss: 0.271091  [38528/60000]\n",
            "loss: 0.309324  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.355176 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.172819  [  128/60000]\n",
            "loss: 0.221062  [12928/60000]\n",
            "loss: 0.263076  [25728/60000]\n",
            "loss: 0.266214  [38528/60000]\n",
            "loss: 0.301857  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.353491 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.163916  [  128/60000]\n",
            "loss: 0.213346  [12928/60000]\n",
            "loss: 0.262011  [25728/60000]\n",
            "loss: 0.261424  [38528/60000]\n",
            "loss: 0.290393  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.351204 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.161695  [  128/60000]\n",
            "loss: 0.210396  [12928/60000]\n",
            "loss: 0.258725  [25728/60000]\n",
            "loss: 0.253769  [38528/60000]\n",
            "loss: 0.285581  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.350679 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.157986  [  128/60000]\n",
            "loss: 0.206236  [12928/60000]\n",
            "loss: 0.256236  [25728/60000]\n",
            "loss: 0.248833  [38528/60000]\n",
            "loss: 0.277592  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.345647 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.150354  [  128/60000]\n",
            "loss: 0.209043  [12928/60000]\n",
            "loss: 0.253367  [25728/60000]\n",
            "loss: 0.246802  [38528/60000]\n",
            "loss: 0.275829  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.344796 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.147241  [  128/60000]\n",
            "loss: 0.203082  [12928/60000]\n",
            "loss: 0.252244  [25728/60000]\n",
            "loss: 0.242477  [38528/60000]\n",
            "loss: 0.267851  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.345349 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.149555  [  128/60000]\n",
            "loss: 0.198085  [12928/60000]\n",
            "loss: 0.244617  [25728/60000]\n",
            "loss: 0.235359  [38528/60000]\n",
            "loss: 0.262954  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.341211 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.146682  [  128/60000]\n",
            "loss: 0.199249  [12928/60000]\n",
            "loss: 0.244490  [25728/60000]\n",
            "loss: 0.228708  [38528/60000]\n",
            "loss: 0.256469  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.340325 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.146407  [  128/60000]\n",
            "loss: 0.193318  [12928/60000]\n",
            "loss: 0.236799  [25728/60000]\n",
            "loss: 0.227144  [38528/60000]\n",
            "loss: 0.251640  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.339316 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.143431  [  128/60000]\n",
            "loss: 0.185395  [12928/60000]\n",
            "loss: 0.234731  [25728/60000]\n",
            "loss: 0.225842  [38528/60000]\n",
            "loss: 0.243148  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.340621 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.147641  [  128/60000]\n",
            "loss: 0.185112  [12928/60000]\n",
            "loss: 0.231572  [25728/60000]\n",
            "loss: 0.221042  [38528/60000]\n",
            "loss: 0.237170  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.338564 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.147091  [  128/60000]\n",
            "loss: 0.181801  [12928/60000]\n",
            "loss: 0.227276  [25728/60000]\n",
            "loss: 0.214036  [38528/60000]\n",
            "loss: 0.236710  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.336170 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.143511  [  128/60000]\n",
            "loss: 0.175524  [12928/60000]\n",
            "loss: 0.222243  [25728/60000]\n",
            "loss: 0.212084  [38528/60000]\n",
            "loss: 0.226688  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.339574 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.142551  [  128/60000]\n",
            "loss: 0.171814  [12928/60000]\n",
            "loss: 0.221494  [25728/60000]\n",
            "loss: 0.207070  [38528/60000]\n",
            "loss: 0.223720  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.339365 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.146104  [  128/60000]\n",
            "loss: 0.173303  [12928/60000]\n",
            "loss: 0.217924  [25728/60000]\n",
            "loss: 0.202843  [38528/60000]\n",
            "loss: 0.217186  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.341331 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.145273  [  128/60000]\n",
            "loss: 0.168470  [12928/60000]\n",
            "loss: 0.216687  [25728/60000]\n",
            "loss: 0.203258  [38528/60000]\n",
            "loss: 0.215505  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.339496 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.144947  [  128/60000]\n",
            "loss: 0.170035  [12928/60000]\n",
            "loss: 0.211643  [25728/60000]\n",
            "loss: 0.199840  [38528/60000]\n",
            "loss: 0.203746  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.339269 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.141810  [  128/60000]\n",
            "loss: 0.166409  [12928/60000]\n",
            "loss: 0.215022  [25728/60000]\n",
            "loss: 0.200218  [38528/60000]\n",
            "loss: 0.201699  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.339880 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.140235  [  128/60000]\n",
            "loss: 0.165086  [12928/60000]\n",
            "loss: 0.212906  [25728/60000]\n",
            "loss: 0.193475  [38528/60000]\n",
            "loss: 0.199825  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.341255 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.139075  [  128/60000]\n",
            "loss: 0.167515  [12928/60000]\n",
            "loss: 0.205774  [25728/60000]\n",
            "loss: 0.188773  [38528/60000]\n",
            "loss: 0.193126  [51328/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.343201 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.134667  [  128/60000]\n",
            "loss: 0.165195  [12928/60000]\n",
            "loss: 0.204918  [25728/60000]\n",
            "loss: 0.188751  [38528/60000]\n",
            "loss: 0.192453  [51328/60000]\n"
          ]
        }
      ],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, lossFn, optimizer1)\n",
        "    test_loop(test_dataloader, model, lossFn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6Y3rt0ZvNBD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnGGD1FMvnl9"
      },
      "outputs": [],
      "source": [
        "logsPath = \"/content/drive/MyDrive/Colab Notebooks/MTCS-205/Assignments/\"\n",
        "writer = SummaryWriter(logsPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb-wX4qXV4dA"
      },
      "outputs": [],
      "source": [
        "randomVector = torch.randn(28*28,)\n",
        "randomVector = randomVector.unsqueeze(0)\n",
        "writer.add_graph(model,randomVector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf6Ru4DcwfXn"
      },
      "outputs": [],
      "source": [
        "!tensorboard --logdir \"/content/drive/MyDrive/Colab Notebooks/MTCS-205/Assignments/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M3KyXs71-ne"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1e-8FOQ4eFPxTNrdsS6bS_EV3Ruwrt6Qy",
      "authorship_tag": "ABX9TyN5eZaCiYlsW/oS0LropKDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}